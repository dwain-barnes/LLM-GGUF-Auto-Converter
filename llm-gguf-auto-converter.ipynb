{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fD24jJxq7t3k"
   },
   "outputs": [],
   "source": [
    "# Cell 1: Configuration and Package Installation\n",
    "\"\"\"\n",
    "This cell sets up the basic configuration and installs required packages.\n",
    "MODEL_ID: The Hugging Face model ID to convert\n",
    "USERNAME: Your Hugging Face username\n",
    "HF_TOKEN: Your Hugging Face API token\n",
    "\"\"\"\n",
    "MODEL_ID = \"model-id-here\" # the mdodel you want to download and convert \n",
    "USERNAME = \"your-username\"  #your hugging face username \n",
    "HF_TOKEN = \"your-token-here\" #your token here\n",
    "MODEL_NAME = MODEL_ID.split('/')[-1]\n",
    "\n",
    "# Install required packages silently\n",
    "!pip install huggingface_hub --quiet --progress-bar off\n",
    "!pip install --upgrade numpy==1.23.5 transformers --quiet --progress-bar off\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import fnmatch\n",
    "from huggingface_hub import create_repo, HfApi, ModelCard, snapshot_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dgpUfAFi8pDX"
   },
   "outputs": [],
   "source": [
    "# Cell 2: Initialize API and Download Model\n",
    "\"\"\"\n",
    "This cell initializes the Hugging Face API and downloads the base model,\n",
    "excluding unnecessary file types to save space and time.\n",
    "\"\"\"\n",
    "hf_token = HF_TOKEN\n",
    "api = HfApi()\n",
    "\n",
    "model_path = snapshot_download(\n",
    "    repo_id=MODEL_ID,\n",
    "    token=hf_token,\n",
    "    ignore_patterns=[\"*.msgpack\", \"*.h5\", \"*.ot\", \"*.onnx\"],\n",
    "    local_dir=MODEL_NAME\n",
    ")\n",
    "print(f\"Model downloaded to: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JqSwLUMH8pVY"
   },
   "outputs": [],
   "source": [
    "# Cell 3: Define Upload Function\n",
    "def upload_quant(base_model_id, quantised_model_name, quantisation_type, save_folder, allow_patterns=None, bpw=None):\n",
    "    \"\"\"\n",
    "    Create a model card (if necessary) and upload the quantised model to Hugging Face.\n",
    "\n",
    "    Parameters:\n",
    "    base_model_id: The ID of the base model\n",
    "    quantised_model_name: The name for the quantised model\n",
    "    quantisation_type: The type of quantisation (e.g., 'gguf', 'gptq', 'awq')\n",
    "    save_folder: The folder where the quantised model is saved\n",
    "    allow_patterns: A list of file patterns to upload\n",
    "    bpw: Bits per weight (used for EXL2 quantisation)\n",
    "    \"\"\"\n",
    "    if quantisation_type == 'exl2':\n",
    "        repo_id = f\"{USERNAME}/{quantised_model_name}-{bpw:.1f}bpw-exl2\"\n",
    "    else:\n",
    "        repo_id = f\"{USERNAME}/{quantised_model_name}\"\n",
    "\n",
    "    try:\n",
    "        existing_card = ModelCard.load(repo_id)\n",
    "        print(f\"Model card already exists for {repo_id}. Skipping model card creation.\")\n",
    "    except Exception:\n",
    "        card = ModelCard.load(base_model_id)\n",
    "        card.data.tags = [] if card.data.tags is None else card.data.tags\n",
    "        card.data.tags.append(\"autoquant\")\n",
    "        card.data.tags.append(quantisation_type)\n",
    "        card.save(f'{save_folder}/README.md')\n",
    "        print(f\"Created new model card for {repo_id}\")\n",
    "\n",
    "    create_repo(\n",
    "        repo_id=repo_id,\n",
    "        repo_type=\"model\",\n",
    "        exist_ok=True,\n",
    "        token=hf_token\n",
    "    )\n",
    "\n",
    "    api.upload_folder(\n",
    "        folder_path=save_folder,\n",
    "        repo_id=repo_id,\n",
    "        allow_patterns=allow_patterns,\n",
    "        token=hf_token\n",
    "    )\n",
    "\n",
    "    print(f\"Uploaded quantised model to {repo_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "NL0yGhbe3EFk"
   },
   "outputs": [],
   "source": [
    "# Cell 4: Setup Quantization Parameters and Install llama.cpp\n",
    "\"\"\"\n",
    "Set up quantization formats and build llama.cpp with CUDA support\n",
    "\"\"\"\n",
    "QUANTISATION_FORMAT = \"q2_k, q3_k_m, q4_k_m, q5_k_m, q6_k, q8_0\"\n",
    "QUANTISATION_METHODS = QUANTISATION_FORMAT.replace(\" \", \"\").split(\",\")\n",
    "gguf_repo_id = f\"{USERNAME}/{MODEL_NAME}-GGUF\"\n",
    "\n",
    "# Install and build llama.cpp with CUDA support\n",
    "if not os.path.exists(\"llama.cpp\"):\n",
    "    !git clone https://github.com/ggerganov/llama.cpp\n",
    "    os.chdir(\"llama.cpp\")\n",
    "    !cmake -B build -DGGML_CUDA=ON\n",
    "    !cmake --build build --config Release\n",
    "    !pip install -r requirements.txt\n",
    "    os.chdir(\"..\")\n",
    "else:\n",
    "    os.chdir(\"llama.cpp\")\n",
    "    # Update build\n",
    "    !cmake -B build -DGGML_CUDA=ON\n",
    "    !cmake --build build --config Release\n",
    "    os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "OE_R3AXG5Y-F"
   },
   "outputs": [],
   "source": [
    "# Cell 5: Convert Model to BF16\n",
    "\"\"\"\n",
    "Convert the model to BF16 format as base for quantization\n",
    "\"\"\"\n",
    "out = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.bf16.gguf\"\n",
    "if os.path.exists(out):\n",
    "    print(f\"File {out} already exists. Skipping conversion.\")\n",
    "else:\n",
    "    if os.getcwd().endswith(\"llama.cpp\"):\n",
    "        os.chdir(\"..\")\n",
    "    !python llama.cpp/convert_hf_to_gguf.py {MODEL_NAME} --outfile {out} --outtype bf16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "ZC9Nsr9u5WhN"
   },
   "outputs": [],
   "source": [
    "# Cell 6: Quantize Model\n",
    "\"\"\"\n",
    "Create different quantized versions of the model\n",
    "\"\"\"\n",
    "for method in QUANTISATION_METHODS:\n",
    "    qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf\"\n",
    "    if not os.getcwd().endswith(\"llama.cpp\"):\n",
    "        os.chdir(\"llama.cpp\")\n",
    "    !./build/bin/llama-quantize ../{out} ../{qtype} {method}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "MyyUO2Fj3WHt"
   },
   "outputs": [],
   "source": [
    "# Cell 7: Upload Quantized Versions\n",
    "\"\"\"\n",
    "Upload all quantized versions to Hugging Face\n",
    "\"\"\"\n",
    "if os.getcwd().endswith(\"llama.cpp\"):\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "upload_quant(\n",
    "    base_model_id=MODEL_ID,\n",
    "    quantised_model_name=f\"{MODEL_NAME}-GGUF\",\n",
    "    quantisation_type=\"gguf\",\n",
    "    save_folder=MODEL_NAME,\n",
    "    allow_patterns=[\"*.gguf\", \"*.md\"]\n",
    ")\n",
    "\n",
    "print(\"All tasks completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "iEhLsUjcnNR7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
